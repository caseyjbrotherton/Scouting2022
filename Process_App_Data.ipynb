{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80444078-3b23-478d-86a4-387e06b2241a",
   "metadata": {},
   "source": [
    "# Process App Data\n",
    "\n",
    " This notebook will read data collected from 2729 Storm Robotics' Scouting Radar\n",
    "and create match and event summaries based on Spamalytics 2022 rankings \n",
    " https://github.com/2729StormRobotics/ScoutingRadar2022 and\n",
    " http://bit.ly/SPAMalytics2022 respectively.\n",
    "\n",
    "*TODO: Consider how we should handle dirty data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea93aa69-2343-48aa-8ee3-0f225fd7cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARN) # restart the kernel after changing level\n",
    "logger = logging.getLogger('Process_App')\n",
    "\n",
    "# Climb point lookup\n",
    "CLIMB_POINTS = dict([\n",
    "    ('Traversal',15), # TBA terminology\n",
    "    ('Traverse',15), # initial app config\n",
    "    ('High',10),\n",
    "    ('Mid',6),\n",
    "    ('Low',4),\n",
    "    ('None',0)])\n",
    "\n",
    "# Create an empty dataframe with the data that we will need.\n",
    "# These columns will determine an id of a set of measurements.\n",
    "ID_COLUMNS = [\"team\", \"match\"]\n",
    "# These columns all contain time that an event occured\n",
    "VALUE_COLUMNS = [\"Taxi\", \"Upper_Hub\", \"Lower_Hub\", \"Miss\", \"Start_Climb\", \"End_Climb\"]\n",
    "# These columns all contain a string that is not a time.\n",
    "DESC_COLUMNS = [\"Endgame_Position\", \"is_red\"]\n",
    "columns_required = ID_COLUMNS + VALUE_COLUMNS + DESC_COLUMNS\n",
    "df = pd.DataFrame(columns=columns_required)\n",
    "\n",
    "# Read all of the CSVs available in the current directory, and combine them\n",
    "for objective_file in glob.glob(\"objective*.csv\"):\n",
    "     temp = pd.read_csv(objective_file,na_filter=False)\n",
    "     df = pd.concat([df,temp])\n",
    "if ( df.empty ):\n",
    "    logger.error('Something went wrong, we read no data.')\n",
    "    raise SystemExit(\"Need to stop\")\n",
    "   \n",
    "# If incremental copies of data occured, we could have duplicate lines of data.\n",
    "# Safe to delete these duplicates\n",
    "df.drop_duplicates( inplace = True)\n",
    "\n",
    "# If scouting errors occurred, we could have duplicates across a team/match combination.\n",
    "# We should print a message, and halt.  Problem needs to be fixed before we continue.\n",
    "df_dup = df[df.duplicated( subset = [\"team\",\"match\"], keep = False)]\n",
    "if ( not df_dup.empty ):\n",
    "     logger.error('There are duplicate entries for team/match with different results.')\n",
    "     logger.debug(df_dup.sort_values( by = [\"team\",\"match\"] ))\n",
    "     raise SystemExit(\"Need to stop\")\n",
    "     \n",
    "# Concat will set columns to NA if they don't exist in one of the concatenated dataframes.\n",
    "# We can subset the dataframe to include just the required columns, \n",
    "# check each of the columns in each row, and return a dataframe with True in each row/column if the value is NA.\n",
    "# and finally check for any True value across the entire dataframe \n",
    "# Otherwise something went wrong with the data collection, and best to throw an error now.\n",
    "missing_columns = pd.isna(df[columns_required])\n",
    "if ( missing_columns.any( axis = None)):\n",
    "     logger.error(\"Missing data in one of the required columns\" )\n",
    "     missing_columns_series = missing_columns.any()\n",
    "     logger.debug(\"Column list with at least one missing entry:\")\n",
    "     logger.debug( missing_columns_series[missing_columns_series] ) # print only columns in the series that have a value of True.\n",
    "     raise SystemExit(\"Need to stop\")\n",
    "        \n",
    "# TODO: Check and see if there are combinations that do not match TBA's match list.\n",
    "# TODO: Consider whether we can continue with above problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f69c1f62-a2b9-416a-bcc2-f06d7362d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data to be one row per match/team/event/time.  \n",
    "# Makes it easier to aggregate counts \n",
    "# Could be removed later if data stream changes.\n",
    "events = df.melt( id_vars = ID_COLUMNS,\n",
    "               value_vars = VALUE_COLUMNS,\n",
    "               var_name = 'event', \n",
    "               value_name = 'time')\n",
    "events = events.assign(time=events.time.str.split(\",\")).explode('time')\n",
    "\n",
    "events = events[ events['time'] != '' ]\n",
    "\n",
    "desc = df[ID_COLUMNS + DESC_COLUMNS]\n",
    "desc=desc.rename(columns={\"Endgame_Position\" : \"endgame_position\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "899f52b8-e990-44fc-abfe-dae62883b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate times into an integer to make easier comparisons.\n",
    "events['time'] = events['time'].astype(int)\n",
    "# create a new value for teleop, and auto that we can use to easily group by and aggregate\n",
    "events['auto'] = '_teleop'\n",
    "events.loc[events['time'] < 15, 'auto'] = '_auto'\n",
    "# create a new key that designated teleop vs auto as well as lowercase to fit in with rest of code\n",
    "events['combo'] = events['event'].apply( lambda x: x.lower()) + events['auto']\n",
    "\n",
    "# Count all events that occured in a match, and merge them with text columns that were excluded out of events\n",
    "summary_events=events.groupby(ID_COLUMNS + ['combo']).agg( \n",
    "    counts = pd.NamedAgg(column='time', aggfunc='count' ))\n",
    "match_summary=summary_events.pivot_table( index = ID_COLUMNS, columns = 'combo', values = 'counts' )\n",
    "match_summary=match_summary.merge(desc, on = ID_COLUMNS, how = 'left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4de26d6a-aaec-4c21-b639-e698e2baf6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate Spamlytics rankings for the data we have been collecting.\n",
    "#  ( http://bit.ly/SPAMalytics2022 )\n",
    "#  Tracks with the following : Team Number,Match Number,Starting Location,\n",
    "#  Robot Preload (1) ,Taxi (2),Auto Cargo Acquired (1),Auto Cargo High Goal (3),Auto Cargo Low Goal (1),Auto Cargo Dropped (-1)\n",
    "#  Cargo Acquired (0.5), Cargo High Goal (1.5), Cargo Low Goal (0.5), Cargo Dropped (-0.5) \n",
    "#  Defensive Performed (2), Defense Encountered (0), Hangar Attempt(0), Hang Level (15,10,6,4,0), Fouls (-3), Technical Fouls (-15)\n",
    "#  We won't track Acquire, so remove penalty for miss, and bonus for acquire.  We also don't track fouls or defensive\n",
    "\n",
    "# TODO: Better way to do this?  Maybe a dictionary of score lookups, and looping through columns?\n",
    "# That would make it reuseable\n",
    "match_summary=match_summary.fillna(0)\n",
    "\n",
    "match_summary['hanger_points']=match_summary.apply( lambda row: CLIMB_POINTS[row.endgame_position], axis=1)\n",
    "match_summary['scouting_points']= match_summary.apply( lambda row: \n",
    "    row.taxi_auto*2 + row.upper_hub_auto*4 + row.lower_hub_auto*2 +\n",
    "    row.upper_hub_teleop*2 + row.lower_hub_teleop*1 + \n",
    "    row.hanger_points, axis=1)\n",
    "\n",
    "event_summary=match_summary.groupby('team').agg('mean')\n",
    "logger.debug(event_summary.sort_values(by='scouting_points',ascending=False).head())\n",
    "logger.debug(match_summary[match_summary['team']==2158])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3994551-e40f-4af8-9c25-55343f693c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write both event summary ,and match summary to a csv for further analysis.\n",
    "event_summary.sort_values(by='scouting_points',ascending=False).to_csv('app_event_summary.csv')\n",
    "match_summary.to_csv('app_match_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d2dfd-a883-4cf4-8986-b19e3721095b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
